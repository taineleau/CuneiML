{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique times found: ['Ur III (ca. 2100-2000 BC)', 'ED IIIb (ca. 2500-2340 BC)', 'Old Akkadian (ca. 2340-2200 BC)', 'Lagash II (ca. 2200-2100 BC)', 'Early Old Babylonian (ca. 2000-1900 BC)', 'Neo-Babylonian (ca. 626-539 BC)', 'Old Babylonian (ca. 1900-1600 BC)', 'ED I-II (ca. 2900-2700 BC)', 'ED IIIa (ca. 2600-2500 BC)', 'Middle Assyrian (ca. 1400-1000 BC)', 'Neo-Assyrian (ca. 911-612 BC)', 'Old Assyrian (ca. 1950-1850 BC)', 'Middle Babylonian (ca. 1400-1100 BC)', 'Ebla (ca. 2350-2250 BC)']\n",
      "Time mapping: {'Ur III (ca. 2100-2000 BC)': 0, 'ED IIIb (ca. 2500-2340 BC)': 1, 'Old Akkadian (ca. 2340-2200 BC)': 2, 'Lagash II (ca. 2200-2100 BC)': 3, 'Early Old Babylonian (ca. 2000-1900 BC)': 4, 'Neo-Babylonian (ca. 626-539 BC)': 5, 'Old Babylonian (ca. 1900-1600 BC)': 6, 'ED I-II (ca. 2900-2700 BC)': 7, 'ED IIIa (ca. 2600-2500 BC)': 8, 'Middle Assyrian (ca. 1400-1000 BC)': 9, 'Neo-Assyrian (ca. 911-612 BC)': 10, 'Old Assyrian (ca. 1950-1850 BC)': 11, 'Middle Babylonian (ca. 1400-1100 BC)': 12, 'Ebla (ca. 2350-2250 BC)': 13}\n",
      "Unique collections found: [516, 6, 139, 215, 874, 17, 352, 71, 40, 931, 277, 1306, 672, 429, 666, 231, 120, 37, 722, 1185, 994, 32, 1418, 210, 494, 829, 656, 584, 683, 293, 1189, 868, 840, 813, 831, 787, 388, 734, 647, 571, 1100, 1023, 603, 202, 690, 1259, 1256, 1251, 1062, 41, 626, 330, 771, 646, 1246, 334, 458, 344, 1136, 405, 684, 1201, 785, 617, 1205, 510, 144, 616, 236, 128, 531, 892, 247, 1265, 1073, 517, 64, 51, 974, 809, 999, 19, 723, 984, 715, 29, 204, 565, 1009, 649, 62, 1224, 224, 154, 946, 1120, 383, 1300, 553, 685, 1072, 1089, 216, 535, 701, 102, 919, 1266, 260, 117, 351, 477, 1207, 581, 302, 677, 1272, 326, 186, 1312, 1166, 898, 1127, 12345, 515, 399, 766, 362, 1206, 942, 1364, 977, 867, 1000, 1267, 732, 814, 1174, 1099, 1222, 916, 932, 239, 455, 769, 457, 78, 640, 768, 361, 1195, 1263, 90, 657, 900, 415, 68, 655, 1255, 995, 1311, 1064, 1271, 1241, 726, 1071, 116, 308, 147, 160, 697, 53, 1180, 364, 370, 850, 654, 1011, 427, 749, 506, 462, 909, 642, 234, 33, 1164, 1106, 1399, 1274, 566, 1269, 20, 314, 751, 11]\n",
      "Collection mapping: {516: 0, 6: 1, 139: 2, 215: 3, 874: 4, 17: 5, 352: 6, 71: 7, 40: 8, 931: 9, 277: 10, 1306: 11, 672: 12, 429: 13, 666: 14, 231: 15, 120: 16, 37: 17, 722: 18, 1185: 19, 994: 20, 32: 21, 1418: 22, 210: 23, 494: 24, 829: 25, 656: 26, 584: 27, 683: 28, 293: 29, 1189: 30, 868: 31, 840: 32, 813: 33, 831: 34, 787: 35, 388: 36, 734: 37, 647: 38, 571: 39, 1100: 40, 1023: 41, 603: 42, 202: 43, 690: 44, 1259: 45, 1256: 46, 1251: 47, 1062: 48, 41: 49, 626: 50, 330: 51, 771: 52, 646: 53, 1246: 54, 334: 55, 458: 56, 344: 57, 1136: 58, 405: 59, 684: 60, 1201: 61, 785: 62, 617: 63, 1205: 64, 510: 65, 144: 66, 616: 67, 236: 68, 128: 69, 531: 70, 892: 71, 247: 72, 1265: 73, 1073: 74, 517: 75, 64: 76, 51: 77, 974: 78, 809: 79, 999: 80, 19: 81, 723: 82, 984: 83, 715: 84, 29: 85, 204: 86, 565: 87, 1009: 88, 649: 89, 62: 90, 1224: 91, 224: 92, 154: 93, 946: 94, 1120: 95, 383: 96, 1300: 97, 553: 98, 685: 99, 1072: 100, 1089: 101, 216: 102, 535: 103, 701: 104, 102: 105, 919: 106, 1266: 107, 260: 108, 117: 109, 351: 110, 477: 111, 1207: 112, 581: 113, 302: 114, 677: 115, 1272: 116, 326: 117, 186: 118, 1312: 119, 1166: 120, 898: 121, 1127: 122, 12345: 123, 515: 124, 399: 125, 766: 126, 362: 127, 1206: 128, 942: 129, 1364: 130, 977: 131, 867: 132, 1000: 133, 1267: 134, 732: 135, 814: 136, 1174: 137, 1099: 138, 1222: 139, 916: 140, 932: 141, 239: 142, 455: 143, 769: 144, 457: 145, 78: 146, 640: 147, 768: 148, 361: 149, 1195: 150, 1263: 151, 90: 152, 657: 153, 900: 154, 415: 155, 68: 156, 655: 157, 1255: 158, 995: 159, 1311: 160, 1064: 161, 1271: 162, 1241: 163, 726: 164, 1071: 165, 116: 166, 308: 167, 147: 168, 160: 169, 697: 170, 53: 171, 1180: 172, 364: 173, 370: 174, 850: 175, 654: 176, 1011: 177, 427: 178, 749: 179, 506: 180, 462: 181, 909: 182, 642: 183, 234: 184, 33: 185, 1164: 186, 1106: 187, 1399: 188, 1274: 189, 566: 190, 1269: 191, 20: 192, 314: 193, 751: 194, 11: 195}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "# File paths\n",
    "file_paths = [\n",
    "    '/graft3/code/tracy/data/final_may24_ver2/test_data_2.json',\n",
    "    '/graft3/code/tracy/data/final_may24_ver2/test_data_3.json',\n",
    "    '/graft3/code/tracy/data/final_may24_ver2/test_data.json',\n",
    "    '/graft3/code/tracy/data/final_may24_ver2/train_data.json',\n",
    "    '/graft3/code/tracy/data/final_may24_ver2/valid_data.json'\n",
    "]\n",
    "\n",
    "# Store all unique time strings\n",
    "all_times = defaultdict(int)\n",
    "\n",
    "# Read JSON files and collect all time information\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for item in data.values():\n",
    "            all_times[item['time']] += 1\n",
    "\n",
    "# Print all unique time strings\n",
    "unique_times = list(all_times.keys())\n",
    "print(\"Unique times found:\", unique_times)\n",
    "\n",
    "# Create time mapping\n",
    "time_mapping = {time: idx for idx, time in enumerate(unique_times)}\n",
    "print(\"Time mapping:\", time_mapping)\n",
    "\n",
    "# Store all unique collection strings\n",
    "all_collections = defaultdict(int)\n",
    "\n",
    "# Read JSON files and collect all collection information\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for item in data.values():\n",
    "            all_collections[item['collection_id']] += 1\n",
    "\n",
    "# Print all unique collection  strings\n",
    "unique_collections = list(all_collections.keys())\n",
    "print(\"Unique collections found:\", unique_collections)\n",
    "\n",
    "# Create time mapping\n",
    "collection_mapping = {collection: idx for idx, collection in enumerate(unique_collections)}\n",
    "print(\"Collection mapping:\", collection_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories.csv has been generated at /graft2/code/yufei/Ancient_Artifact_Dating_front/wilds/data/iwildcam_v2.0/categories.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Read categories.csv and create a category mapping\n",
    "output_categories_csv_path = '/graft2/code/yufei/Ancient_Artifact_Dating_front/wilds/data/iwildcam_v2.0/categories.csv'\n",
    "\n",
    "\n",
    "# Generate categories.csv file\n",
    "with open(output_categories_csv_path, 'w', newline='') as categories_csvfile:\n",
    "    fieldnames = ['y', 'category_id', 'name']\n",
    "    writer = csv.DictWriter(categories_csvfile, fieldnames=fieldnames)\n",
    "    \n",
    "    writer.writeheader()\n",
    "    \n",
    "    for time_string, idx in time_mapping.items():\n",
    "        writer.writerow({\n",
    "            'y': idx,\n",
    "            'category_id': idx,  # Assuming category_id is the same as y for simplicity\n",
    "            'name': time_string\n",
    "        })\n",
    "\n",
    "print(f\"categories.csv has been generated at {output_categories_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata.csv has been generated at /graft2/code/yufei/Ancient_Artifact_Dating_front/wilds/data/iwildcam_v2.0/metadata.csv\n",
      "Number of files in train: 30109\n",
      "Number of files in val: 2114\n",
      "Number of files in id_test: 1958\n",
      "Number of files in id_val: 2037\n",
      "Number of files in test: 2065\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Specify the output path for metadata.csv\n",
    "output_csv_path = '/graft2/code/yufei/Ancient_Artifact_Dating_front/wilds/data/iwildcam_v2.0/metadata.csv'\n",
    "\n",
    "split_counters = {\n",
    "    'train': 0,\n",
    "    'val': 0,\n",
    "    'id_test': 0,\n",
    "    'id_val': 0,\n",
    "    'test': 0\n",
    "}\n",
    "\n",
    "with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['split', 'location_remapped', 'location', 'sequence_remapped', 'seq_id', 'y', 'category_id', 'datetime', 'filename', 'image_id']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    \n",
    "    writer.writeheader()\n",
    "    \n",
    "    missing = 0\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            # Determine the split based on the file name\n",
    "            if 'train' in file_path:\n",
    "                split = 'train'\n",
    "            elif 'valid' in file_path:\n",
    "                split = 'val'\n",
    "            elif 'test_data_2' in file_path:\n",
    "                split = 'id_test'\n",
    "            elif 'test_data_3' in file_path:\n",
    "                split = 'id_val'\n",
    "            elif 'test_data' in file_path:\n",
    "                split = 'test'\n",
    "            \n",
    "            for item in data.values():\n",
    "                image_id = item['id']\n",
    "                filename = item['img_url'].split('/')[-1]\n",
    "                if len(str(image_id)) <= 5:  # Skip if image_id is six digits\n",
    "                    missing = missing + 1\n",
    "                    filename = f\"P{image_id:06d}.jpg\"\n",
    "                    # print(filename)\n",
    "                    continue  # Uncomment this line to actually skip the images\n",
    "                \n",
    "                location_remapped_mapping = lambda x: x\n",
    "                writer.writerow({\n",
    "                    'split': split,\n",
    "                    'location_remapped': location_remapped_mapping(item['collection_id']),  # Use the same value\n",
    "                    'location': collection_mapping[item['collection_id']],\n",
    "                    'sequence_remapped': 1,  # Use the same value\n",
    "                    'seq_id': image_id,\n",
    "                    'y': time_mapping[item['time']],  # Encode time string as a numerical value\n",
    "                    'category_id': time_mapping[item['time']],\n",
    "                    'datetime': '2100-01-01 00:00:00.000',  # Use the same value\n",
    "                    'filename': filename,  # Use the image file name\n",
    "                    'image_id': image_id\n",
    "                })\n",
    "                \n",
    "                # Increment the counter for the current split\n",
    "                split_counters[split] += 1\n",
    "\n",
    "# print(\"Img missing: \", missing)\n",
    "print(f\"metadata.csv has been generated at {output_csv_path}\")\n",
    "\n",
    "# Print the count of files for each split\n",
    "for split, count in split_counters.items():\n",
    "    print(f\"Number of files in {split}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import json\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Load metadata\n",
    "# metadata_path = '/trunk2/yufei/Ancient_Artifact_Dating_front/wilds/data/iwildcam_v2.0/metadata.csv'\n",
    "# metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "# # Load valid_data.json and test_data.json\n",
    "# with open('/trunk2/yufei/Ancient_Artifact_Dating_front/wilds/data/mydataset/collection/valid_data.json', 'r') as f:\n",
    "#     valid_data = json.load(f)\n",
    "# with open('/trunk2/yufei/Ancient_Artifact_Dating_front/wilds/data/mydataset/collection/test_data.json', 'r') as f:\n",
    "#     test_data = json.load(f)\n",
    "\n",
    "# # Count the occurrences of each time (mapped to y) in valid_data and test_data\n",
    "# valid_y_counts = pd.Series([item['time'] for item in valid_data.values()]).map(time_mapping).value_counts()\n",
    "# test_y_counts = pd.Series([item['time'] for item in test_data.values()]).map(time_mapping).value_counts()\n",
    "\n",
    "# # Separate train data\n",
    "# train_data = metadata[metadata['split'] == 'train']\n",
    "\n",
    "# # Function to sample data based on y distribution\n",
    "# def stratified_sample(data, counts):\n",
    "#     sampled_data = pd.DataFrame()\n",
    "#     for y, count in counts.items():\n",
    "#         y_data = data[data['y'] == y]\n",
    "#         if len(y_data) >= count:\n",
    "#             sampled_y_data = y_data.sample(n=count, random_state=42)\n",
    "#         else:\n",
    "#             sampled_y_data = y_data\n",
    "#         sampled_data = pd.concat([sampled_data, sampled_y_data])\n",
    "#     return sampled_data\n",
    "\n",
    "# # Sample id_val and id_test from train_data\n",
    "# id_val_data = stratified_sample(train_data, valid_y_counts)\n",
    "# # Avoid overlap\n",
    "# id_test_data = stratified_sample(train_data.drop(id_val_data.index), test_y_counts)\n",
    "\n",
    "# # Assign new splits\n",
    "# id_val_data['split'] = 'id_val'\n",
    "# id_test_data['split'] = 'id_test'\n",
    "\n",
    "# # Remove the sampled data from train_data\n",
    "# train_data = train_data.drop(id_val_data.index).drop(id_test_data.index)\n",
    "\n",
    "# # Concatenate the new datasets with the original metadata\n",
    "# new_metadata = pd.concat([metadata[metadata['split'] != 'train'], train_data, id_val_data, id_test_data])\n",
    "\n",
    "# # Save the new metadata\n",
    "# new_metadata_path = '/trunk2/yufei/Ancient_Artifact_Dating_front/wilds/data/iwildcam_v2.0/metadata.csv'\n",
    "# new_metadata.to_csv(new_metadata_path, index=False)\n",
    "\n",
    "# print(f\"New metadata.csv has been generated at {new_metadata_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files in metadata.csv are present in the images directory.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# Specify file paths\n",
    "metadata_path = '/graft2/code/yufei/Ancient_Artifact_Dating_front/wilds/data/iwildcam_v2.0/metadata.csv'\n",
    "images_dir = '/graft2/datasets/danlu/cuneiform/segmented_images/segmented_images'\n",
    "\n",
    "# Read the metadata.csv file\n",
    "missing_files = []\n",
    "with open(metadata_path, 'r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        filename = row['filename']\n",
    "        file_path = os.path.join(images_dir, filename)\n",
    "        if not os.path.isfile(file_path):\n",
    "            missing_files.append(filename)\n",
    "\n",
    "# Print missing files\n",
    "if missing_files:\n",
    "    print(f\"Missing files ({len(missing_files)}):\")\n",
    "    for file in missing_files:\n",
    "        print(file)\n",
    "else:\n",
    "    print(\"All files in metadata.csv are present in the images directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
